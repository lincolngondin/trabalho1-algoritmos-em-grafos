This work introduces a new cubic regularization method for nonconvex unconstrained multiobjective optimization problems. At each iteration of the method, a model associated with the cubic regularization of each component of the objective function is minimized. This model allows approximations for the first- and second-order derivatives, which must satisfy suitable error conditions. One interesting feature of the proposed algorithm is that the regularization parameter of the model and the accuracy of the derivative approximations are jointly adjusted using a nonmonotone line search criterion. Implementations of the method, where derivative information is computed using finite difference strategies, are discussed. It is shown that, under the assumption that the Hessians of the objectives are globally Lipschitz continuous, the method requires at most $\mathcal{O}(C\epsilon^{-3/2})$ iterations to generate an $\epsilon$-approximate Pareto critical. In particular, if the first- and second-order derivative information is computed using finite differences based solely on function values, the method requires at most $\mathcal{O}(n^{1-\beta}\epsilon^{-3/2})$ iterations, corresponding to $\mathcal{O}\left(mn^{3-\beta}\varepsilon^{-\frac{3}{2}}\right)$ function evaluations, where \(n\) is the dimension of the domain of the objective function, $m$ is the number of objectives, and $\beta \in [0,1]$ is a constant associated with the stepsize used in the finite-difference approximation. We further discuss the global convergence and local convergence rate of the method. Specifically, under the local convexity assumption, we show that the method achieves superlinear convergence when the first derivative is computed exactly, and quadratic convergence when both first- and second-order derivatives are exact.