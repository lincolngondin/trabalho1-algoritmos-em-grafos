We propose a generalization of the stochastic Krasnoselskil-Mann $(SKM)$ algorithm to reflexive Banach spaces endowed with Bregman distances. Under standard martingale-difference noise assumptions in the dual space and mild conditions on the distance-generating function, we establish almost-sure convergence to a fixed point and derive non-asymptotic residual bounds that depend on the uniform convexity modulus of the generating function. Extensions to adaptive Bregman geometries and robust noise models are also discussed. Numerical experiments on entropy-regularized reinforcement learning and mirror-descent illustrate the theoretical findings.