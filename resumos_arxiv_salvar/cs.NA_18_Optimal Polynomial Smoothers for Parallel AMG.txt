In this paper, we explore polynomial accelerators that are well-suited for parallel computations, specifically as smoothers in Algebraic MultiGrid (AMG) preconditioners. These accelerators address a minimax problem, initially formulated in [Lottes, Numer. Lin. Alg. with Appl. 30(6), 2518 (2023)], aiming to achieve an optimal (or near-optimal) bound for a polynomial-dependent constant involved in the AMG V-cycle error bound, without requiring information about the matrices' spectra. Lottes focuses on Chebyshev polynomials of the 4th-kind and defines the relevant recurrence formulas applicable to a general convergent basic smoother. In this paper, we demonstrate the efficacy of these accelerations for large-scale applications on modern GPU-accelerated supercomputers. Furthermore, we formulate a variant of the aforementioned minimax problem, which naturally leads to solutions relying on Chebyshev polynomials of the 1st-kind as accelerators for a basic smoother. For all the polynomial accelerations, we describe efficient GPU kernels for their application and demonstrate their comparable effectiveness on standard benchmarks at very large scales.